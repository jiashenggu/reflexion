{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'torchdata' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n torchdata ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "from FlagEmbedding import FlagModel\n",
    "sentences_1 = [\"样例数据-1\", \"样例数据-2\"]\n",
    "sentences_2 = [\"样例数据-3\", \"样例数据-4\"]\n",
    "model = FlagModel('/ML-A100/home/gujiasheng/bge-small-en', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages: \",\n",
    "                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "embeddings_1 = model.encode(sentences_1)\n",
    "embeddings_2 = model.encode(sentences_2)\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)\n",
    "\n",
    "# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n",
    "# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\n",
    "queries = ['good']\n",
    "passages = [\"This is a good sentence\", \"This is a bad sentence\"]\n",
    "q_embeddings = model.encode_queries(queries)\n",
    "p_embeddings = model.encode(passages)\n",
    "scores = q_embeddings @ p_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 384)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8341131 0.754635 ]\n"
     ]
    }
   ],
   "source": [
    "for s in scores:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://hf-mirror.com'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('HF_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gujiasheng/miniconda3/envs/swe-bench/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2023-11-24 13:44:35,167 WARNING Disabling caching\n",
      "/home/gujiasheng/miniconda3/envs/swe-bench/lib/python3.9/site-packages/ghapi/core.py:102: UserWarning: Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated\n",
      "  else: warn('Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b098f5b15f8a4d48b108a992043d9b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 13:44:45,531 INFO Creating instance pytorch__data-1169\n",
      "2023-11-24 13:44:45,532 INFO Cloning repo pytorch/data\n",
      "2023-11-24 13:44:45,558 INFO Buidling BM25 retrieval index for pytorch/data@a5b4720dece60565788ac4c9a85e01719188b28e\n",
      "2023-11-24 13:44:45,721 INFO Retrieved 3 documents\n",
      "2023-11-24 13:44:45,762 INFO Including 3 files in context with 3332 tokens:\n",
      "scripts/release_notes/commitlist.py____6\n",
      "\ttorchdata/dataloader2/communication/iter.py____10\n",
      "\ttorchdata/dataloader2/communication/map.py____5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchdata/dataloader2/communication/map.py____5\n",
      "scripts/release_notes/commitlist.py____6\n",
      "torchdata/dataloader2/communication/iter.py____10\n"
     ]
    }
   ],
   "source": [
    "from run_live import retrieve_repo\n",
    "args = {\n",
    "    \"query\": \"good\",\n",
    "    \"prompt_style\": \"style-3\",\n",
    "    \"issue_url\": [\"https://github.com/pytorch/data/issues/1169\"],\n",
    "    \"base_commit\": [None],\n",
    "    \"max_context_length\": 4096,\n",
    "    \"document_encoding_func\": \"file_name_and_contents\",\n",
    "    \"root_dir\": \"./run_live_data\",\n",
    "    \"include_readmes\": False\n",
    "}\n",
    "\n",
    "instance = retrieve_repo(**args)\n",
    "retrieved_functions = \"\"\n",
    "for v in instance['file_contents'].values():\n",
    "    retrieved_functions += v + \"\\n\\n\\n\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def DataPipeBehindQueues(\n",
      "    source_datapipe,\n",
      "    protocol,\n",
      "    process_name,\n",
      "    loop_id,\n",
      "    worker_info,\n",
      "    custom_reset_fn,\n",
      "    blocking_request_get=False,\n",
      "    request_counter=None,\n",
      "):\n",
      "    \"\"\"\n",
      "    Indefinitely iterates over req_queue and passing values from source_datapipe to res_queue.\n",
      "\n",
      "    Args:\n",
      "        source_datapipe: DataPipe\n",
      "        protocol: ``MapDataPipeQueueProtocolServer`` that contains ``req_queue`` and ``res_queue``\n",
      "        process_name: Process name\n",
      "        loop_id: Loop ID\n",
      "        worker_info: Worker info include worker id and number of workers\n",
      "        custom_reset_fn: function to call after each request is received\n",
      "        blocking_request_get: determines if ``protocol.get_new_request`` will block\n",
      "    \"\"\"\n",
      "    if not isinstance(protocol, communication.protocol.MapDataPipeQueueProtocolServer):\n",
      "        raise Exception(\"Expecting MapDataPipeQueueProtocolServer, got\", protocol)\n",
      "    source_datapipe = EnsureNonBlockingMapDataPipe(source_datapipe)\n",
      "    forever = True\n",
      "    while forever:\n",
      "        try:\n",
      "            # TODO: non-blocking call is extremely slow here for python.mp, need to figure out a good workaround\n",
      "            request = protocol.get_new_request(block=blocking_request_get)\n",
      "        except communication.protocol.EmptyQueue:\n",
      "            yield True\n",
      "            continue\n",
      "\n",
      "        if isinstance(request, communication.messages.ResetEpochRequest):\n",
      "            distributed_shared_seed = request_counter is not None\n",
      "            source_datapipe = process_reset_fn(\n",
      "                source_datapipe,\n",
      "                worker_info,\n",
      "                request.seed_generator,\n",
      "                distributed_shared_seed,\n",
      "                request.iter_reset_fn,\n",
      "                custom_reset_fn,\n",
      "            )\n",
      "            protocol.response_reset_epoch()\n",
      "\n",
      "        elif isinstance(request, communication.messages.TerminateRequest):\n",
      "            forever = False\n",
      "            protocol.response_terminate()\n",
      "\n",
      "        elif isinstance(request, communication.messages.LenRequest):\n",
      "            size = source_datapipe.nonblocking_len()\n",
      "            protocol.response_len(size)\n",
      "\n",
      "        elif isinstance(request, communication.messages.GetItemRequest):\n",
      "            while forever:\n",
      "                try:\n",
      "                    value = source_datapipe.nonblocking_getitem(request.key)\n",
      "                except NotAvailable:\n",
      "                    yield True\n",
      "                    continue\n",
      "                except IndexError:\n",
      "                    # Alternatively, we can just allow the underlying DataPipe to throw an exception?\n",
      "                    protocol.response_index_out_of_bound()\n",
      "                    yield True\n",
      "                    break\n",
      "                except Exception:\n",
      "                    exc = ExceptionWrapper(where=f\"in {process_name} {loop_id}\")\n",
      "                    protocol.response_worker_exception(exc)\n",
      "                    break\n",
      "                protocol.response_item(request.key, value)\n",
      "                yield True  # Returns control\n",
      "                break\n",
      "        else:\n",
      "            raise Exception(\"Unrecognized type of request received\", request)\n",
      "def get_markdown_header(category):\n",
      "    header = f\"\"\"\n",
      "# Release Notes worksheet {category}\n",
      "The main goal of this process is to rephrase all the commit messages below to make them clear and easy to read by the end user. You should follow the following instructions to do so:\n",
      "* **Please cleanup, and format commit titles to be readable by the general pytorch user.** [Detailed intructions here](https://fb.quip.com/OCRoAbEvrRD9#HdaACARZZvo)\n",
      "* Please sort commits into the following categories (you should not rename the categories!), I tried to pre-sort these to ease your work, feel free to move commits around if the current categorization is not good.\n",
      "* Please drop any commits that are not user-facing.\n",
      "* If anything is from another domain, leave it in the UNTOPICED section at the end and I'll come and take care of it.\n",
      "The categories below are as follows:\n",
      "* BC breaking: All commits that are BC-breaking. These are the most important commits. If any pre-sorted commit is actually BC-breaking, do move it to this section. Each commit should contain a paragraph explaining the rational behind the change as well as an example for how to update user code (guidelines here: https://quip.com/OCRoAbEvrRD9)\n",
      "* Deprecations: All commits introducing deprecation. Each commit should include a small example explaining what should be done to update user code.\n",
      "* new_features: All commits introducing a new feature (new functions, new submodule, new supported platform etc)\n",
      "* improvements: All commits providing improvements to existing feature should be here (new backend for a function, new argument, better numerical stability)\n",
      "* bug fixes: All commits that fix bugs and behaviors that do not match the documentation\n",
      "* performance: All commits that are added mainly for performance (we separate this from improvements above to make it easier for users to look for it)\n",
      "* documentation: All commits that add/update documentation\n",
      "* Developers: All commits that are not end-user facing but still impact people that compile from source, develop into pytorch, extend pytorch, etc\n",
      "\"\"\"\n",
      "\n",
      "    return [\n",
      "        header,\n",
      "    ]\n",
      "def DataPipeBehindQueues(\n",
      "    source_datapipe,\n",
      "    protocol,\n",
      "    process_name,\n",
      "    loop_id,\n",
      "    worker_info,\n",
      "    custom_reset_fn,\n",
      "    blocking_request_get=False,\n",
      "    request_counter=None,\n",
      "):\n",
      "    \"\"\"\n",
      "    Indefinitely iterates over ``req_queue`` and passing values from source_datapipe to ``res_queue``.\n",
      "\n",
      "    Request Types:\n",
      "        `ResetEpoch` - Call the `reset_epoch_fn` on the protocol's DataPipe and reset DataPipe iterator\n",
      "        `Terminate` - exits the infinite while loop\n",
      "        `GetNext` - returns the value from the DataPipe, and handles exceptions such as `StopIteration` as appropriate\n",
      "        `Limit` - Set limit to the DataPipe graph\n",
      "        `Pause` - Pause\n",
      "        the DataPipe graph\n",
      "        `Resume` - Resume the DataPipe graph\n",
      "\n",
      "    Args:\n",
      "        source_datapipe: DataPipe\n",
      "        protocol: ``IterDataPipeQueueProtocolServer`` that contains ``req_queue`` and ``res_queue``\n",
      "        process_name: Process name\n",
      "        loop_id: Loop ID\n",
      "        worker_info: Worker info include worker id and number of workers\n",
      "        custom_reset_fn: function to call after each request is received\n",
      "        blocking_request_get: determines if ``protocol.get_new_request`` will block\n",
      "        request_counter: Optional counter to synchronize all loops that have received requests for\n",
      "            reset/limit/pause/resume within the dispatching process. It would guarantee that\n",
      "            all loops starts to reset iterator and get next element at the same time.\n",
      "    \"\"\"\n",
      "    if not isinstance(protocol, communication.protocol.IterDataPipeQueueProtocolServer):\n",
      "        raise Exception(\"Expecting IterDataPipeQueueProtocolServer, got\", protocol)\n",
      "    source_datapipe = EnsureNonBlockingDataPipe(source_datapipe)\n",
      "    forever = True\n",
      "    while forever:\n",
      "        try:\n",
      "            # TODO: Non-blocking call is extremely slow here for python.mp, need to figure out a good workaround\n",
      "            request = protocol.get_new_request(block=blocking_request_get)\n",
      "        except communication.protocol.EmptyQueue:\n",
      "            yield True\n",
      "            continue\n",
      "\n",
      "        # TODO: Handle Error caused by requests other than GetNext and send it to main process\n",
      "        if isinstance(request, communication.messages.ResetEpochRequest):\n",
      "            yield from _sync_recv(request_counter, \"reset_epoch\")\n",
      "            distributed_shared_seed = request_counter is not None\n",
      "            if request_counter is None or loop_id == 0:\n",
      "                seed_generator = request.seed_generator\n",
      "                iter_reset_fn = request.iter_reset_fn\n",
      "                dispatching_dps = find_dps(traverse_dps(source_datapipe), _IterateQueueDataPipes)\n",
      "                for dp in dispatching_dps:\n",
      "                    dp.reset_epoch(seed_generator, iter_reset_fn)\n",
      "                source_datapipe = process_reset_fn(\n",
      "                    source_datapipe,\n",
      "                    worker_info,\n",
      "                    seed_generator,\n",
      "                    distributed_shared_seed,\n",
      "                    iter_reset_fn,\n",
      "                    custom_reset_fn,\n",
      "                )\n",
      "            source_datapipe.reset_iterator()\n",
      "            yield from _sync_resp(request_counter, \"reset_epoch\")\n",
      "            protocol.response_reset_epoch()\n",
      "            yield True  # Returns control\n",
      "\n",
      "        elif isinstance(request, communication.messages.LimitRequest):\n",
      "            yield from _sync_recv(request_counter, \"limit\")\n",
      "            if request_counter is None or loop_id == 0:\n",
      "                num_batches = request.num_batches\n",
      "                limit_fn = request.limit_fn\n",
      "                worker_num_batches = num_batches if request.worker_num_batches is None else request.worker_num_batches\n",
      "                # Send limit to the worker/dispatching process\n",
      "                dispatching_dps = find_dps(traverse_dps(source_datapipe), _IterateQueueDataPipes)\n",
      "                for dp in dispatching_dps:\n",
      "                    dp.request_limit(num_batches, limit_fn, worker_num_batches)\n",
      "                if limit_fn is not None:\n",
      "                    # Set limit to the DataPipe graph in worker/dispatching process\n",
      "                    source_datapipe = limit_fn(source_datapipe, worker_num_batches)\n",
      "            yield from _sync_resp(request_counter, \"limit\")\n",
      "            protocol.response_limit()\n",
      "            yield True  # Returns control\n",
      "\n",
      "        elif isinstance(request, communication.messages.PauseRequest):\n",
      "            yield from _sync_recv(request_counter, \"pause\")\n",
      "            if request_counter is None or loop_id == 0:\n",
      "                graph = traverse_dps(source_datapipe)\n",
      "                dp_list = list_dps(graph)\n",
      "                for dp in dp_list:\n",
      "                    if hasattr(dp, \"pause\") and callable(dp.pause):\n",
      "                        dp.pause()\n",
      "                dispatching_dps = find_dps(graph, _IterateQueueDataPipes)\n",
      "                for dp in dispatching_dps:\n",
      "                    dp.request_pause(request.pause_fn)\n",
      "                if request.pause_fn is not None:\n",
      "                    source_datapipe = request.pause_fn(source_datapipe)\n",
      "            yield from _sync_resp(request_counter, \"pause\")\n",
      "            protocol.response_pause()\n",
      "            yield True  # Returns control\n",
      "\n",
      "        elif isinstance(request, communication.messages.ResumeRequest):\n",
      "            yield from _sync_recv(request_counter, \"resume\")\n",
      "            if request_counter is None or loop_id == 0:\n",
      "                if request.resume_fn is not None:\n",
      "                    source_datapipe = request.resume_fn(source_datapipe)\n",
      "                graph = traverse_dps(source_datapipe)\n",
      "                # Send resume to the dispatching process\n",
      "                dispatching_dps = find_dps(graph, _IterateQueueDataPipes)\n",
      "                for dp in dispatching_dps:\n",
      "                    dp.request_resume(request.resume_fn)\n",
      "                for dp in reversed(list_dps(graph)):\n",
      "                    if hasattr(dp, \"resume\") and callable(dp.resume):\n",
      "                        dp.resume()\n",
      "            yield from _sync_resp(request_counter, \"resume\")\n",
      "            protocol.response_resume()\n",
      "            yield True  # Returns control\n",
      "\n",
      "        elif isinstance(request, communication.messages.TerminateRequest):\n",
      "            forever = False\n",
      "            dispatch_dps = find_dps(traverse_dps(source_datapipe), _IterateQueueDataPipes)\n",
      "            for dispatch_dp in dispatch_dps:\n",
      "                dispatch_dp.request_terminate()\n",
      "            protocol.response_terminate()\n",
      "            yield True  # Returns control\n",
      "\n",
      "        elif isinstance(request, communication.messages.GetNextRequest):\n",
      "            while forever:\n",
      "                if protocol.is_paused():\n",
      "                    protocol.response_stop_iteration()\n",
      "                    warnings.warn(\n",
      "                        \"Cannot `GetNext` after `Pause` has been called. \"\n",
      "                        \"`Resume` must be called first before additional elements can be yielded.\"\n",
      "                    )\n",
      "                    yield True\n",
      "                    break\n",
      "                try:\n",
      "                    value = source_datapipe.nonblocking_next()\n",
      "                except NotAvailable:\n",
      "                    yield True\n",
      "                    continue\n",
      "                except StopIteration:\n",
      "                    protocol.response_stop_iteration()\n",
      "                    yield True\n",
      "                    break\n",
      "                except InvalidStateResetRequired:\n",
      "                    protocol.response_invalid_state()\n",
      "                    yield True\n",
      "                    break\n",
      "                except Exception:\n",
      "                    exc = ExceptionWrapper(where=f\"in {process_name} {loop_id}\")\n",
      "                    protocol.response_worker_exception(exc)\n",
      "                    return\n",
      "                protocol.response_next(value)\n",
      "                yield True  # Returns control\n",
      "                break\n",
      "        else:\n",
      "            raise Exception(\"Unrecognized type of request received\", request)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [start]\n",
      "# flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.\n",
      "# cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times.\n",
      "# mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.\n",
      "# header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.\n",
      "# concat(*args, **kwds): Concatenates multiple Iterable DataPipes.\n",
      "# [end]\n",
      "from torchdata.datapipes.iter import IterableWrapper\n",
      "datapipe = IterableWrapper([1,2,3])\n",
      "# How to augument the datapipe by repeating it six times.\n",
      "new_datapipe =\n"
     ]
    }
   ],
   "source": [
    "print(\"# [start]\\n# flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.\\n# cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times.\\n# mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.\\n# header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.\\n# concat(*args, **kwds): Concatenates multiple Iterable DataPipes.\\n# [end]\\nfrom torchdata.datapipes.iter import IterableWrapper\\ndatapipe = IterableWrapper([1,2,3])\\n# How to augument the datapipe by repeating it six times.\\nnew_datapipe =\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retrieved_apis]:\n",
      "{retrieved_apis}\n",
      "\n",
      "[retrieved_functions]:\n",
      "{retrieved_functions}\n",
      "\n",
      "{func_sig}\n"
     ]
    }
   ],
   "source": [
    "print('[retrieved_apis]:\\n{retrieved_apis}\\n\\n[retrieved_functions]:\\n{retrieved_functions}\\n\\n{func_sig}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
