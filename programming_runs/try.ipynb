{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'torchdata' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n torchdata ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "from FlagEmbedding import FlagModel\n",
    "sentences_1 = [\"样例数据-1\", \"样例数据-2\"]\n",
    "sentences_2 = [\"样例数据-3\", \"样例数据-4\"]\n",
    "model = FlagModel('/ML-A100/home/gujiasheng/bge-small-en', \n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages: \",\n",
    "                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "embeddings_1 = model.encode(sentences_1)\n",
    "embeddings_2 = model.encode(sentences_2)\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)\n",
    "\n",
    "# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n",
    "# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\n",
    "queries = ['good']\n",
    "passages = [\"This is a good sentence\", \"This is a bad sentence\"]\n",
    "q_embeddings = model.encode_queries(queries)\n",
    "p_embeddings = model.encode(passages)\n",
    "scores = q_embeddings @ p_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 384)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8341131 0.754635 ]\n"
     ]
    }
   ],
   "source": [
    "for s in scores:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://hf-mirror.com'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('HF_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gujiasheng/miniconda3/envs/swe-bench/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2023-11-24 13:44:35,167 WARNING Disabling caching\n",
      "/home/gujiasheng/miniconda3/envs/swe-bench/lib/python3.9/site-packages/ghapi/core.py:102: UserWarning: Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated\n",
      "  else: warn('Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b098f5b15f8a4d48b108a992043d9b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 13:44:45,531 INFO Creating instance pytorch__data-1169\n",
      "2023-11-24 13:44:45,532 INFO Cloning repo pytorch/data\n",
      "2023-11-24 13:44:45,558 INFO Buidling BM25 retrieval index for pytorch/data@a5b4720dece60565788ac4c9a85e01719188b28e\n",
      "2023-11-24 13:44:45,721 INFO Retrieved 3 documents\n",
      "2023-11-24 13:44:45,762 INFO Including 3 files in context with 3332 tokens:\n",
      "scripts/release_notes/commitlist.py____6\n",
      "\ttorchdata/dataloader2/communication/iter.py____10\n",
      "\ttorchdata/dataloader2/communication/map.py____5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchdata/dataloader2/communication/map.py____5\n",
      "scripts/release_notes/commitlist.py____6\n",
      "torchdata/dataloader2/communication/iter.py____10\n"
     ]
    }
   ],
   "source": [
    "from run_live import retrieve_repo\n",
    "args = {\n",
    "    \"query\": \"good\",\n",
    "    \"prompt_style\": \"style-3\",\n",
    "    \"issue_url\": [\"https://github.com/pytorch/data/issues/1169\"],\n",
    "    \"base_commit\": [None],\n",
    "    \"max_context_length\": 4096,\n",
    "    \"document_encoding_func\": \"file_name_and_contents\",\n",
    "    \"root_dir\": \"./run_live_data\",\n",
    "    \"include_readmes\": False\n",
    "}\n",
    "\n",
    "instance = retrieve_repo(**args)\n",
    "retrieved_functions = \"\"\n",
    "for v in instance['file_contents'].values():\n",
    "    retrieved_functions += v + \"\\n\\n\\n\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_path: torchdata.datapipes.iter.batch\n",
      "api_name: batch\n",
      "api_doc: \n",
      "    Creates mini-batches of data (functional name: ``batch``). An outer dimension will be added as\n",
      "    ``batch_size`` if ``drop_last`` is set to ``True``, or ``length % batch_size`` for the\n",
      "    last batch if ``drop_last`` is set to ``False``.\n",
      "\n",
      "    Args:\n",
      "        datapipe: Iterable DataPipe being batched\n",
      "        batch_size: The size of each batch\n",
      "        drop_last: Option to drop the last batch if it's not full\n",
      "        wrapper_class: wrapper to apply onto each batch (type ``List``) before yielding,\n",
      "            defaults to ``DataChunk``\n",
      "\n",
      "    Example:\n",
      "        >>> from torchdata.datapipes.iter import IterableWrapper\n",
      "        >>> dp = IterableWrapper(range(10))\n",
      "        >>> dp = dp.batch(batch_size=3, drop_last=True)\n",
      "        >>> list(dp)\n",
      "        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
      "    \n",
      "api_signature: (datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List)\n",
      "api_description: Creates mini-batches of data. An outer dimension will be added as\n",
      "    ``batch_size`` if ``drop_last`` is set to ``True``, or ``length % batch_size`` for the\n",
      "    last batch if ``drop_last`` is set to ``False``.\n",
      "api_parameters: \n",
      "datapipe: Iterable DataPipe being batched\n",
      "        batch_size: The size of each batch\n",
      "        drop_last: Option to drop the last batch if it's not full\n",
      "        wrapper_class: wrapper to apply onto each batch (type ``List``) before yielding,\n",
      "            defaults to ``DataChunk``\n",
      "api_parameters_number: =2\n",
      "api_returns: \n",
      "api_see_also: \n",
      "api_notes: \n",
      "api_examples: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"api_path: torchdata.datapipes.iter.batch\\napi_name: batch\\napi_doc: \\n    Creates mini-batches of data (functional name: ``batch``). An outer dimension will be added as\\n    ``batch_size`` if ``drop_last`` is set to ``True``, or ``length % batch_size`` for the\\n    last batch if ``drop_last`` is set to ``False``.\\n\\n    Args:\\n        datapipe: Iterable DataPipe being batched\\n        batch_size: The size of each batch\\n        drop_last: Option to drop the last batch if it's not full\\n        wrapper_class: wrapper to apply onto each batch (type ``List``) before yielding,\\n            defaults to ``DataChunk``\\n\\n    Example:\\n        >>> from torchdata.datapipes.iter import IterableWrapper\\n        >>> dp = IterableWrapper(range(10))\\n        >>> dp = dp.batch(batch_size=3, drop_last=True)\\n        >>> list(dp)\\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\\n    \\napi_signature: (datapipe: IterDataPipe, batch_size: int, drop_last: bool = False, wrapper_class=List)\\napi_description: Creates mini-batches of data. An outer dimension will be added as\\n    ``batch_size`` if ``drop_last`` is set to ``True``, or ``length % batch_size`` for the\\n    last batch if ``drop_last`` is set to ``False``.\\napi_parameters: \\ndatapipe: Iterable DataPipe being batched\\n        batch_size: The size of each batch\\n        drop_last: Option to drop the last batch if it's not full\\n        wrapper_class: wrapper to apply onto each batch (type ``List``) before yielding,\\n            defaults to ``DataChunk``\\napi_parameters_number: =2\\napi_returns: \\napi_see_also: \\napi_notes: \\napi_examples: \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [start]\n",
      "# flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.\n",
      "# cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times.\n",
      "# mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.\n",
      "# header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.\n",
      "# concat(*args, **kwds): Concatenates multiple Iterable DataPipes.\n",
      "# [end]\n",
      "from torchdata.datapipes.iter import IterableWrapper\n",
      "datapipe = IterableWrapper([1,2,3])\n",
      "# How to augument the datapipe by repeating it six times.\n",
      "new_datapipe =\n"
     ]
    }
   ],
   "source": [
    "print(\"# [start]\\n# flatmap(*args, **kwds): Applies a function over each item from the source DataPipe, then flattens the outputs to a single, unnested IterDataPipe.\\n# cycle(*args, **kwds): Cycles the specified input in perpetuity by default, or for the specified number of times.\\n# mux(*datapipes): Yields one element at a time from each of the input Iterable DataPipes.\\n# header(source_datapipe: IterDataPipe[torchdata.datapipes.iter.util.header.T_co], limit: int = 10): Yields elements from the source DataPipe from the start, up to the specfied limit.\\n# concat(*args, **kwds): Concatenates multiple Iterable DataPipes.\\n# [end]\\nfrom torchdata.datapipes.iter import IterableWrapper\\ndatapipe = IterableWrapper([1,2,3])\\n# How to augument the datapipe by repeating it six times.\\nnew_datapipe =\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retrieved_apis]:\n",
      "{retrieved_apis}\n",
      "\n",
      "[retrieved_functions]:\n",
      "{retrieved_functions}\n",
      "\n",
      "{func_sig}\n"
     ]
    }
   ],
   "source": [
    "print('[retrieved_apis]:\\n{retrieved_apis}\\n\\n[retrieved_functions]:\\n{retrieved_functions}\\n\\n{func_sig}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
